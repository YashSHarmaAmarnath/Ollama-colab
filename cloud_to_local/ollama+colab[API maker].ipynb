{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GznbTqmVO3h"
      },
      "source": [
        "# Colab + Ollama  \n",
        "---\n",
        "## Install ollama on colab and use cloud gpu to run any Open source **L**arge **L**anguage **M**odel\n",
        "---\n",
        "### steps to follow before installing ollama\n",
        "- change connection type from CPU to T4 GPU\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BKPGRbTPIh9",
        "outputId": "155b8cde-71e8-4fc4-dcb3-d4e7f8d9f063"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "zstd is already the newest version (1.4.8+dfsg-3build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y zstd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUJHLC8lVTq2"
      },
      "source": [
        "## Install ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqMzgvNVPLNp",
        "outputId": "fdaa04ca-758e-48f2-8579-21c277ef96ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading ollama-linux-amd64.tar.zst\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!curl -fssL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jt_90Yc1POZ7",
        "outputId": "216a2c61-dd44-427a-fc93-81bce846c9f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ollama in /usr/local/lib/python3.12/dist-packages (0.6.1)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.12/dist-packages (from ollama) (2.12.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.4.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HCtnIMh_PRrb"
      },
      "outputs": [],
      "source": [
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lzf-oIAhPTr5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WwOSMTocPXGi"
      },
      "outputs": [],
      "source": [
        "os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "os.environ['OLLAMA_ORIGINS'] = '*'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZLPSdjDPZEQ",
        "outputId": "3c53038a-c5ef-4988-b2e7-bf36072cf474"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Popen: returncode: None args: ['nohup', 'ollama', 'serve']>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "subprocess.Popen([\"nohup\", \"ollama\", \"serve\"],\n",
        "                 stdout=open(\"ollama.log\", \"a\"),\n",
        "                 stderr=open(\"ollama.err\", \"a\"),\n",
        "                 preexec_fn=os.setpgrp)\n",
        "time.sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-t4iXosVdeW"
      },
      "source": [
        "---\n",
        "run this command\n",
        "`ollama pull <model-name>`  to install LLM or run `ollama run <model-name>` to install and run  \\\n",
        " Replace `<model-name>` with llm name you want to install, I will be installing Gemma 3\n",
        "### List of llms from ollama GitHub\n",
        "\n",
        "## Model library\n",
        "\n",
        "Ollama supports a list of models available on [ollama.com/library](https://ollama.com/library \"ollama model library\")\n",
        "\n",
        "Here are some example models that can be downloaded:\n",
        "\n",
        "| Model              | Parameters | Size  | Download                         |\n",
        "| ------------------ | ---------- | ----- | -------------------------------- |\n",
        "| Gemma 3            | 1B         | 815MB | `ollama run gemma3:1b`           |\n",
        "| Gemma 3            | 4B         | 3.3GB | `ollama run gemma3`              |\n",
        "| Gemma 3            | 12B        | 8.1GB | `ollama run gemma3:12b`          |\n",
        "| Gemma 3            | 27B        | 17GB  | `ollama run gemma3:27b`          |\n",
        "| QwQ                | 32B        | 20GB  | `ollama run qwq`                 |\n",
        "| DeepSeek-R1        | 7B         | 4.7GB | `ollama run deepseek-r1`         |\n",
        "| DeepSeek-R1        | 671B       | 404GB | `ollama run deepseek-r1:671b`    |\n",
        "| Llama 4            | 109B       | 67GB  | `ollama run llama4:scout`        |\n",
        "| Llama 4            | 400B       | 245GB | `ollama run llama4:maverick`     |\n",
        "| Llama 3.3          | 70B        | 43GB  | `ollama run llama3.3`            |\n",
        "| Llama 3.2          | 3B         | 2.0GB | `ollama run llama3.2`            |\n",
        "| Llama 3.2          | 1B         | 1.3GB | `ollama run llama3.2:1b`         |\n",
        "| Llama 3.2 Vision   | 11B        | 7.9GB | `ollama run llama3.2-vision`     |\n",
        "| Llama 3.2 Vision   | 90B        | 55GB  | `ollama run llama3.2-vision:90b` |\n",
        "| Llama 3.1          | 8B         | 4.7GB | `ollama run llama3.1`            |\n",
        "| Llama 3.1          | 405B       | 231GB | `ollama run llama3.1:405b`       |\n",
        "| Phi 4              | 14B        | 9.1GB | `ollama run phi4`                |\n",
        "| Phi 4 Mini         | 3.8B       | 2.5GB | `ollama run phi4-mini`           |\n",
        "| Mistral            | 7B         | 4.1GB | `ollama run mistral`             |\n",
        "| Moondream 2        | 1.4B       | 829MB | `ollama run moondream`           |\n",
        "| Neural Chat        | 7B         | 4.1GB | `ollama run neural-chat`         |\n",
        "| Starling           | 7B         | 4.1GB | `ollama run starling-lm`         |\n",
        "| Code Llama         | 7B         | 3.8GB | `ollama run codellama`           |\n",
        "| Llama 2 Uncensored | 7B         | 3.8GB | `ollama run llama2-uncensored`   |\n",
        "| LLaVA              | 7B         | 4.5GB | `ollama run llava`               |\n",
        "| Granite-3.3        | 8B         | 4.9GB | `ollama run granite3.3`          |\n",
        "\n",
        "> [!NOTE]\n",
        "> You should have at least 8 GB of RAM available to run the 7B models, 16 GB to run the 13B models, and 32 GB to run the 33B models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtnBOrm7Pa-1",
        "outputId": "c9ba4d37-9991-4856-eb03-33d13bc12111"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ],
      "source": [
        "!ollama pull gemma3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N15Q4fIIVpH-"
      },
      "source": [
        "## Run following command in terminal\n",
        "`./cloudflared-linux-amd64 tunnel --url http://localhost:1211`\\\n",
        "it will give url, copy url use it to connect this instance\n",
        "\n",
        "> [!]example\n",
        "\n",
        "2026-01-23T11:49:21Z INF +--------------------------------------------------------------------------------------------+\\\n",
        "2026-01-23T11:49:21Z INF |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):|\\\n",
        "2026-01-23T11:49:21Z INF |  https://example.trycloudflare.com| \\\n",
        "2026-01-23T11:49:21Z INF +--------------------------------------------------------------------------------------------+"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pw3DAE9xPhbD",
        "outputId": "932322f2-5ba5-4c48-bf13-8afb8685d872"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[90m2026-02-01T12:49:10Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2026-02-01T12:49:10Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2026-02-01T12:49:14Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2026-02-01T12:49:14Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2026-02-01T12:49:14Z\u001b[0m \u001b[32mINF\u001b[0m |  https://lunch-wednesday-priorities-sin.trycloudflare.com                                  |\n",
            "\u001b[90m2026-02-01T12:49:14Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2026-02-01T12:49:14Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2026-02-01T12:49:14Z\u001b[0m \u001b[32mINF\u001b[0m Version 2026.1.2 (Checksum e157c54e929cc289cbd53860453168c2fe3439eb55e2e965a56579252585d9c1)\n",
            "\u001b[90m2026-02-01T12:49:14Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.11, GoArch: amd64\n",
            "\u001b[90m2026-02-01T12:49:14Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 protocol:quic url:http://localhost:11434]\n",
            "\u001b[90m2026-02-01T12:49:14Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/configure-tunnels/local-management/as-a-service/\n",
            "\u001b[90m2026-02-01T12:49:14Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: 14364661-6989-4dc7-8a69-61fb0dd2e6f6\n",
            "\u001b[90m2026-02-01T12:49:14Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2026-02-01T12:49:14Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2026-02-01T12:49:14Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2026-02-01T12:49:14Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2026-02-01T12:49:14Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2026-02-01T12:49:14Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2026-02-01T12:49:14Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.33\n",
            "2026/02/01 12:49:14 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2026-02-01T12:49:14Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0m273fadfb-e9ad-4ca9-8a6e-d04160ce3f5d \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.33 \u001b[36mlocation=\u001b[0mlax01 \u001b[36mprotocol=\u001b[0mquic\n",
            "\u001b[90m2026-02-01T13:12:24Z\u001b[0m \u001b[32mINF\u001b[0m Initiating graceful shutdown due to signal interrupt ...\n",
            "\u001b[90m2026-02-01T13:12:24Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m failed to run the datagram handler \u001b[31merror=\u001b[0m\u001b[31m\"context canceled\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.33\n",
            "\u001b[90m2026-02-01T13:12:24Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m failed to serve tunnel connection \u001b[31merror=\u001b[0m\u001b[31m\"accept stream listener encountered a failure while serving\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.33\n",
            "\u001b[90m2026-02-01T13:12:24Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Serve tunnel error \u001b[31merror=\u001b[0m\u001b[31m\"accept stream listener encountered a failure while serving\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.33\n",
            "\u001b[90m2026-02-01T13:12:24Z\u001b[0m \u001b[32mINF\u001b[0m Retrying connection in up to 1s \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.33\n",
            "\u001b[90m2026-02-01T13:12:24Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Connection terminated \u001b[36mconnIndex=\u001b[0m0\n",
            "\u001b[90m2026-02-01T13:12:24Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m no more connections active and exiting\n",
            "\u001b[90m2026-02-01T13:12:24Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel server stopped\n",
            "\u001b[90m2026-02-01T13:12:24Z\u001b[0m \u001b[32mINF\u001b[0m Metrics server stopped\n"
          ]
        }
      ],
      "source": [
        "!./cloudflared-linux-amd64 tunnel --url http://localhost:11434"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmaDKpxJT3IY"
      },
      "source": [
        "#ðŸ”— Use remote Ollama server (Cloudflare URL)\n",
        "\n",
        "Take your tunnel URL, for example:\n",
        "\n",
        "https://random-words-1234.trycloudflare.com\n",
        "\n",
        "ðŸ–¥ï¸ Linux / Mac\n",
        "\n",
        "`export OLLAMA_HOST=https://random-words-1234.trycloudflare.com`\n",
        "\n",
        "`ollama run gemma3`\n",
        "\n",
        "ðŸªŸ Windows (PowerShell)\n",
        "\n",
        "`setx OLLAMA_HOST \"https://random-words-1234.trycloudflare.com\"`\n",
        "\n",
        "`ollama run gemma3`\n",
        "\n",
        "\n",
        "Open a new terminal after running that, then:\n",
        "\n",
        "`ollama run gemma3`\n",
        "\n",
        "âœ… What happens now"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fDjDo6iUNWb"
      },
      "source": [
        "## Use it in python\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-QV-KZxUpUw"
      },
      "source": [
        "copy paste following code\n",
        "replcae url with your cloudflare url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCF_If8WPk_9"
      },
      "outputs": [],
      "source": [
        "import ollama\n",
        "\n",
        "def ask_llm(prompt, client,history=None,model='gemma3'):\n",
        "    messages = []\n",
        "    if history:\n",
        "        for u, b in history:\n",
        "            messages.append({\"role\": \"user\", \"content\": u})\n",
        "            messages.append({\"role\": \"assistant\", \"content\": b})\n",
        "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    response = client.chat(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        stream=False\n",
        "    )\n",
        "    return response[\"message\"][\"content\"]\n",
        "\n",
        "def start_chat(client, model='gemma3'):\n",
        "    history = []\n",
        "    while True:\n",
        "        prompt = input(\"\\nask-> \")\n",
        "        if prompt in ('/bye', 'exit'): break\n",
        "        response = ask_llm(prompt=prompt,client=client,history=history,model=model)\n",
        "        print(\"\\n\",response)\n",
        "        history.append((prompt,response))\n",
        "    return history\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # replace it with your cloudflare url\n",
        "    remote_host = \"https://lunch-wednesday-priorities-sin.trycloudflare.com/\"\n",
        "\n",
        "    client = ollama.Client(host=remote_host)\n",
        "\n",
        "    start_chat(client=client,model=\"gemma3\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
