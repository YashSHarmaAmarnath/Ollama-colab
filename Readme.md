# Ollama + Colab

---

This setup lets you run **any LLM** inside **Google Colab**, using the **cloud GPU** provided by Colab.

## Features

* Run Ollama-powered LLMs directly in Colab
* Exposes a **web endpoint** to use the model like a chatbot
* Supports **inline chat** (non-web version)

> ⚠️ **Note**: The non-web (inline chat) version is still under development and is **not included in the repository yet**.

